{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9BpIlqAZci"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Hand gesture recognition model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JO1GUwC1_T2x"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBcmjzf0JLE"
      },
      "source": [
        "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
        "\n",
        "This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGM0PT490LiR"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVVxZNfo0M0y"
      },
      "source": [
        "Install the MediaPipe Model Maker package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6DBLRE-fqlO5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe-model-maker\n",
            "  Using cached mediapipe_model_maker-0.2.1.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: absl-py in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe-model-maker) (2.1.0)\n",
            "Requirement already satisfied: mediapipe>=0.10.0 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe-model-maker) (0.10.13)\n",
            "Requirement already satisfied: numpy in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe-model-maker) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe-model-maker) (4.9.0.80)\n",
            "Collecting tensorflow<2.16,>=2.10 (from mediapipe-model-maker)\n",
            "  Using cached tensorflow-2.15.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
            "Collecting tensorflow-addons (from mediapipe-model-maker)\n",
            "  Using cached tensorflow_addons-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-datasets (from mediapipe-model-maker)\n",
            "  Using cached tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting tensorflow-hub (from mediapipe-model-maker)\n",
            "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-model-optimization<0.8.0 (from mediapipe-model-maker)\n",
            "  Using cached tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-text (from mediapipe-model-maker)\n",
            "  Using cached tensorflow_text-2.17.0rc0-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
            "Collecting tf-models-official<2.16.0,>=2.13.2 (from mediapipe-model-maker)\n",
            "  Using cached tf_models_official-2.15.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (24.3.25)\n",
            "Requirement already satisfied: jax in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.26)\n",
            "Requirement already satisfied: matplotlib in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.8.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.9.0.80)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.25.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.6)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=2.9.0 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.3.0)\n",
            "Requirement already satisfied: packaging in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (24.0)\n",
            "Requirement already satisfied: setuptools in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (4.11.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached wrapt-1.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached grpcio-1.64.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.10->mediapipe-model-maker)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting absl-py (from mediapipe-model-maker)\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting dm-tree~=0.1.1 (from tensorflow-model-optimization<0.8.0->mediapipe-model-maker)\n",
            "  Using cached dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
            "Collecting Cython (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached Cython-3.0.10-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pillow in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (10.3.0)\n",
            "Collecting gin-config (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting google-api-python-client>=1.6.7 (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached google_api_python_client-2.137.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting immutabledict (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting kaggle>=1.3.9 (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached kaggle-1.6.14.tar.gz (82 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting oauth2client (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opencv-python-headless (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
            "Collecting pandas>=0.22.0 (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.9.0)\n",
            "Collecting py-cpuinfo>=3.3.0 (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting pycocotools (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached pycocotools-2.0.8-cp311-cp311-macosx_10_9_universal2.whl.metadata (1.1 kB)\n",
            "Collecting pyyaml>=6.0.0 (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Collecting sacrebleu (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.13.0)\n",
            "Collecting sentencepiece (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Collecting seqeval (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker)\n",
            "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-models-official<2.16.0,>=2.13.2 (from mediapipe-model-maker)\n",
            "  Using cached tf_models_official-2.14.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.14.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.14.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Using cached tf_models_official-2.13.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting mediapipe-model-maker\n",
            "  Using cached mediapipe_model_maker-0.2.1.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting tensorflow>=2.10 (from mediapipe-model-maker)\n",
            "  Using cached tensorflow-2.17.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
            "Collecting tf-models-official>=2.13.1 (from mediapipe-model-maker)\n",
            "  Using cached tf_models_official-2.16.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
            "  Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "INFO: pip is still looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-models-official>=2.13.1 (from mediapipe-model-maker)\n",
            "  Using cached tf_models_official-2.13.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyyaml<5.4.0,>=5.1 (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
            "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting mediapipe-model-maker\n",
            "  Using cached mediapipe_model_maker-0.2.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached mediapipe_model_maker-0.2.1.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached mediapipe_model_maker-0.2.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Using cached mediapipe_model_maker-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting tf-models-official==2.11.6 (from mediapipe-model-maker)\n",
            "  Using cached tf_models_official-2.11.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyyaml<6.0,>=5.1 (from tf-models-official==2.11.6->mediapipe-model-maker)\n",
            "  Using cached PyYAML-5.4.1.tar.gz (175 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[54 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m running egg_info\n",
            "  \u001b[31m   \u001b[0m writing lib3/PyYAML.egg-info/PKG-INFO\n",
            "  \u001b[31m   \u001b[0m writing dependency_links to lib3/PyYAML.egg-info/dependency_links.txt\n",
            "  \u001b[31m   \u001b[0m writing top-level names to lib3/PyYAML.egg-info/top_level.txt\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
            "  \u001b[31m   \u001b[0m     main()\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
            "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/johnathanchiu/anaconda3/envs/hand-pose/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
            "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 327, in get_requires_for_build_wheel\n",
            "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 297, in _get_build_requires\n",
            "  \u001b[31m   \u001b[0m     self.run_setup()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 313, in run_setup\n",
            "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 271, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 103, in setup\n",
            "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
            "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
            "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
            "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 974, in run_command\n",
            "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
            "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 321, in run\n",
            "  \u001b[31m   \u001b[0m     self.find_sources()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 329, in find_sources\n",
            "  \u001b[31m   \u001b[0m     mm.run()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 550, in run\n",
            "  \u001b[31m   \u001b[0m     self.add_defaults()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/command/egg_info.py\", line 588, in add_defaults\n",
            "  \u001b[31m   \u001b[0m     sdist.add_defaults(self)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/command/sdist.py\", line 102, in add_defaults\n",
            "  \u001b[31m   \u001b[0m     super().add_defaults()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/sdist.py\", line 250, in add_defaults\n",
            "  \u001b[31m   \u001b[0m     self._add_defaults_ext()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/sdist.py\", line 335, in _add_defaults_ext\n",
            "  \u001b[31m   \u001b[0m     self.filelist.extend(build_ext.get_source_files())\n",
            "  \u001b[31m   \u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 201, in get_source_files\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/pj/3l6r37v558dg9qvx0ytwsrbw0000gn/T/pip-build-env-b0w9lq55/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 107, in __getattr__\n",
            "  \u001b[31m   \u001b[0m     raise AttributeError(attr)\n",
            "  \u001b[31m   \u001b[0m AttributeError: cython_sources\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CvTNmB1WiY"
      },
      "source": [
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74UL9oI0VKU"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppoENBmAuFn"
      },
      "source": [
        "## Simple End-to-End Example\n",
        "\n",
        "This end-to-end example uses Model Maker to customize a model for on-device gesture recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8fMLXTdD6tW"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwDFilngzjs"
      },
      "source": [
        "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.\n",
        "\n",
        "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4HZHEZJyWEU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dwmyg5MnR_y"
      },
      "outputs": [],
      "source": [
        "# !wget https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip\n",
        "# !unzip rps_data_sample.zip\n",
        "# dataset_path = \"rps_data_sample\"\n",
        "\n",
        "!unzip drive/MyDrive/dataset.zip\n",
        "dataset_path = \"dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiWb9Tu3lBBI"
      },
      "source": [
        "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgadM4VDj3Y2"
      },
      "outputs": [],
      "source": [
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0o59OMjqmV"
      },
      "source": [
        "To better understand the dataset, plot a couple of example images for each gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx8PsrwYjvgO"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES = 5\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(dataset_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXwEXSXlg7d"
      },
      "source": [
        "### Run the example\n",
        "The workflow consists of 4 steps which have been separated into their own code blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9ArLQXIu25"
      },
      "source": [
        "**Load the dataset**\n",
        "\n",
        "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
        "\n",
        "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
        "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
        "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
        "\n",
        "Split the dataset: 80% for training, 10% for validation, and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTNZsolKXiT"
      },
      "outputs": [],
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTh_ZyEIeKV"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAXWc3bv8hpe"
      },
      "source": [
        "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the [Hyperparameters](#hyperparameters) section below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk0UiRB6NZrb"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nED7mdIO9YS6"
      },
      "source": [
        "**Evaluate the model performance**\n",
        "\n",
        "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdOqllqx9YKy"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJLramjy9gvy"
      },
      "source": [
        "**Export to Tensorflow Lite Model**\n",
        "\n",
        "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNaFXytijVg"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yfN_47qjjOC"
      },
      "outputs": [],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulqyNGmTCKeU"
      },
      "source": [
        "## Run the model on-device\n",
        "\n",
        "To use the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer [overview page](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OiY4w9RghOl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tiLGGRcvhy"
      },
      "source": [
        "## Hyperparameters {:#hyperparameters}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1UMEG85hQL_"
      },
      "source": [
        "You can further customize the model using the `GestureRecognizerOptions` class, which has two optional parameters for `ModelOptions` and `HParams`. Use the `ModelOptions` class to customize parameters related to the model itself, and the `HParams` class to customize other parameters related to training and saving the model.\n",
        "\n",
        "`ModelOptions` has one customizable parameter that affects accuracy:\n",
        "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
        "* `layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
        "\n",
        "`HParams` has the following list of customizable parameters which affect model accuracy:\n",
        "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
        "* `batch_size`: Batch size for training. Defaults to 2.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
        "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
        "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
        "* `lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
        "* `gamma`: Gamma parameter for focal loss. Defaults to 2\n",
        "\n",
        "Additional `HParams` parameter that does not affect model accuracy:\n",
        "* `export_dir`: The location of the model checkpoint files and exported model files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psvVZeSYBLfV"
      },
      "source": [
        "For example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxMOI8o6iNLu"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
        "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
        "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
        "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cekuTJiBbv9"
      },
      "source": [
        "Evaluate the newly trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRH96bm-BbAo"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model_2.evaluate(test_data)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRtStJYCzVIQ"
      },
      "outputs": [],
      "source": [
        "model_2.export_model()\n",
        "!ls exported_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
